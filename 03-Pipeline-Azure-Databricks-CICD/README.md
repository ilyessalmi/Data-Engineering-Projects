# Pipeline ETL End-to-End & DevOps sur Azure Databricks

![Azure](https://img.shields.io/badge/Azure-Databricks-HW5252?style=for-the-badge&logo=microsoft-azure) ![Spark](https://img.shields.io/badge/Apache-Spark-E25A1C?style=for-the-badge&logo=apache-spark) ![Python](https://img.shields.io/badge/Python-NLTK%20%7C%20Pandas-3776AB?style=for-the-badge&logo=python) ![GitLab](https://img.shields.io/badge/GitLab-CI%2FCD-FC6D26?style=for-the-badge&logo=gitlab)

## üìã R√©sum√© Ex√©cutif

Ce projet d√©montre la conception et l'impl√©mentation d'un **pipeline de donn√©es complet (End-to-End)** sur le cloud Microsoft Azure. L'objectif √©tait d'ing√©rer des flux de donn√©es non structur√©es (Tweets), de les transformer via un traitement distribu√© (Spark NLP), et d'automatiser le cycle de d√©ploiement via une approche **DevOps (CI/CD)**.

Ce projet met en avant une double comp√©tence :
1.  **Data Engineering :** D√©veloppement de scripts Python/Spark robustes pour l'ingestion et l'analyse de sentiment.
2.  **Cloud Architecture :** Administration de clusters Databricks et orchestration de workflows automatis√©s.

## üõ† Architecture Technique

Le pipeline int√®gre des composants d'ingestion, de traitement massif et de reporting.

```mermaid
graph LR
    %% Styles
    classDef source fill:#1DA1F2,stroke:#fff,stroke-width:2px,color:#fff,rx:5,ry:5;
    classDef azure fill:#0078D4,stroke:#fff,stroke-width:2px,color:#fff,rx:5,ry:5;
    classDef processing fill:#E25A1C,stroke:#fff,stroke-width:2px,color:#fff,rx:5,ry:5;
    classDef viz fill:#F2C811,stroke:#fff,stroke-width:2px,color:#000,rx:5,ry:5;

    subgraph Source ["üì° Ingestion (Data Engineering)"]
        API["API Twitter v2"]:::source
        Script["Script Python (Tweepy)"]:::source
        RawData["Stockage Brut (Raw)"]:::source
    end

    subgraph Azure_Env ["‚òÅÔ∏è Traitement Big Data (Spark/Databricks)"]
        Clean["Data Sanitization (Regex)"]:::processing
        Token["Tokenization & Stop-words"]:::processing
        NLP["Analyse Sentiment (TextBlob)"]:::processing
        Parquet["Stockage Optimis√© (Parquet)"]:::azure
    end

    subgraph Viz ["üìä Business Intelligence"]
        PBI["Power BI Dashboard"]:::viz
    end

    %% Flux
    API -->|Authentification Bearer| Script
    Script -->|Pagination & Quotas| RawData
    RawData -->|Ingestion DBFS| Clean
    Clean -->|Transformation| Token
    Token -->|Spark UDF| NLP
    NLP -->|√âcriture Distribu√©e| Parquet
    Parquet -->|Connecteur ODBC| PBI

    %% Styling
    style Source fill:#f9f9f9,stroke:#666,color:#000
    style Azure_Env fill:#e6f7ff,stroke:#0078D4,color:#000
    style Viz fill:#fff5f0,stroke:#F2C811,color:#000
```


## üíª Impl√©mentation Data Engineering

### 1. Ingestion de Donn√©es (Python & Tweepy)
D√©veloppement d'un script d'extraction robuste g√©rant l'authentification et la pagination des r√©sultats pour contourner les limitations de requ√™tes par d√©faut.
```python
# Extrait du script de collecte (ingest)
def collect_tweets(query, max_results):
    client = tweepy.Client(bearer_token=BEARER_TOKEN)
    tweets = client.search_recent_tweets(
        query=query, 
        max_results=max_results, 
        tweet_fields=['context_annotations', 'created_at']
    )
    # Logique de gestion des quotas et stockage CSV...
    return tweets
```

### 2. Pr√©traitement et Assainissement (Data Sanitization)
Les donn√©es textuelles brutes n√©cessitent un nettoyage rigoureux avant analyse. Utilisation d'expressions r√©guli√®res (Regex) pour normaliser le contenu.
```Python
# Fonction de nettoyage (prepare)
def clean_tweet(tweet):
    # Suppression des URLs
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)
    # Suppression des mentions @User et hashtags #
    tweet = re.sub(r'\@\w+|\#', '', tweet)
    return tweet
```

### 3. Analyse Distribu√©e avec Spark (NLP)
Pour passer √† l'√©chelle sur de gros volumes, l'analyse de sentiment est encapsul√©e dans une Spark UDF (User Defined Function), permettant d'ex√©cuter du code Python sur les n≈ìuds du cluster Spark.
```Python
# Application de l'analyse de sentiment sur DataFrame Spark
from textblob import TextBlob
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def analyze_sentiment(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0: return 'positive'
    elif analysis.sentiment.polarity == 0: return 'neutral'
    return 'negative'

# Enregistrement UDF et ex√©cution
sentiment_udf = udf(analyze_sentiment, StringType())
df_final = df_clean.withColumn("sentiment", sentiment_udf(df_clean.Filtered_Tweet))
```


## ‚öôÔ∏è Administration Cloud & DevOps

### 1. Pipeline CI/CD (GitLab)
Automatisation du d√©ploiement via un fichier .gitlab-ci.yml. Ce pipeline assure que chaque modification du code est test√©e avant d'√™tre d√©ploy√©e sur l'environnement Databricks de production.
```Yaml
stages:
  - test
  - deploy
  - run

test_job:
  stage: test
  script:
    - pip install -r requirements.txt
    - pytest tests/  # Ex√©cution des tests unitaires

deploy_job:
  stage: deploy
  script:
    - databricks workspace import_dir . /Shared/TwitterProject --overwrite
```

### 2. Administration Cluster Azure
* Configuration : Cluster Standard avec Runtime ML (Machine Learning) pour supporter les librairies NLTK/TextBlob.
* FinOps : Mise en place d'une politique d'auto-termination (shutdown) apr√®s 20 minutes d'inactivit√© pour optimiser les co√ªts de consommation Azure.



## üì∏ R√©sultats et Livrables

### 1. Orchestration du Workflow (Databricks Jobs)
Vue de l'ex√©cution s√©quentielle des t√¢ches : Ingestion -> Pr√©paration -> Analyse.

![alt text](databricks-run.png)

### 2. Configuration du Cluster Spark
Param√©trage de l'infrastructure de calcul.

![alt text](databricks-cluster.png)

### 3. Automatisation CI/CD
Pipeline GitLab validant et d√©ployant le code.

![alt text](gitlab-pipeline.png)

### 4. Dashboard Analytique (Power BI)
Restitution visuelle des KPIs de sentiment et nuages de mots.

![alt text](powerbi-dashboard.png)

---

## üöÄ Comp√©tences Acquises
* Ing√©nierie de Donn√©es : Manipulation avanc√©e de DataFrames PySpark et nettoyage de donn√©es non structur√©es.
* Infrastructure as Code (IaC) : Compr√©hension des m√©canismes de d√©ploiement automatis√© (CI/CD).
* Administration Azure : Gestion des ressources de calcul (Clusters), des secrets (API Keys) et de la connectivit√© (ODBC vers Power BI).
